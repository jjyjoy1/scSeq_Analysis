Great, I’ll compile a detailed technical report summarizing advances in single-cell data analysis from the last five years, with a focus on human data. This will include the application of machine learning and deep learning models from sequencing noise correction to cell type annotation, T/B cell repertoire analysis, and spatial transcriptomics. I’ll focus on published methods, model architectures, and commonly used tools.

I’ll let you know as soon as the report is ready.


# Advances in Machine Learning for Single-Cell Data Analysis (2019–2024)

Single-cell genomic technologies have rapidly advanced, yielding complex datasets that challenge traditional analysis methods due to their noise, sparsity, and high dimensionality. In the last five years, machine learning (ML) – especially deep learning (DL) – has been applied at every stage of the single-cell analysis pipeline to extract meaningful biological insights from human single-cell data. Deep neural networks can capture intricate patterns in heterogeneous, noisy single-cell data, often outperforming conventional approaches. Below, we provide a structured review of these advances, covering the analysis pipeline (from noise correction to cell type annotation), immune repertoire analysis for T and B cells, spatial transcriptomics, and the key DL model architectures (autoencoders, graph neural networks, transformers, etc.) that have driven new methods. We highlight widely adopted, peer-reviewed tools and models (with citations) and summarize them in tables for clarity.

## ML and DL Across the Single-Cell Analysis Pipeline

Single-cell RNA sequencing (scRNA-seq) analysis involves multiple steps: data preprocessing (quality control, noise reduction), feature extraction/dimensionality reduction, batch effect correction (dataset integration), clustering and trajectory inference, and cell type annotation. Recent studies have introduced ML/DL methods to improve each of these steps. **Figure 1** outlines an example pipeline, and **Table 1** summarizes representative methods.

### Data Denoising and Imputation

Single-cell data are plagued by dropout events (many zero values) and technical noise. Early approaches (e.g. MAGIC, scImpute) used statistical or KNN-based imputation, but these struggled with the nonlinearity and scale of scRNA-seq data. Deep learning methods have since been developed to “denoise” expression data by predicting and filling in missing or noisy values. For example, **DeepImpute** (2019) uses a deep neural network (DNN) that focuses on subsets of genes to efficiently predict a cell’s missing gene expression values from its other genes. This approach was shown to effectively recover dropout values while remaining scalable by limiting model complexity. Another innovative model, **scIGAN** (2020), introduced a generative adversarial network (GAN) that *simulates* realistic gene expression vectors; the generator network creates “fake” cells to impute dropouts, rather than relying solely on observed data. Numerous autoencoder-based methods have also been applied: an autoencoder (AE) learns a compressed representation of the data and reconstructs the input, and the reconstructed output can serve as a denoised version of the expression matrix. For instance, **DCA (Deep Count Autoencoder)** integrates a **zero-inflated negative binomial (ZINB)** noise model into an autoencoder; instead of directly outputting denoised counts, the decoder predicts parameters of the ZINB distribution for each gene (mean, dispersion, dropout rate). The mean of this output distribution is used as the imputed expression, significantly reducing technical dropout noise. Methods like **scGMAI** and **scScope** use recurrent or iterative autoencoder frameworks to refine imputation and have shown that denoising can improve downstream clustering accuracy. **SAVER-X** (2019) took a unique transfer learning approach: it trains an autoencoder on large public datasets (including cross-species data) to capture gene–gene relationships, then fine-tunes on a new dataset, leveraging prior knowledge to improve imputation in small human datasets. Overall, these deep learning imputation methods (see Table 1) have demonstrated improved recovery of gene expression patterns from noisy data, providing a better foundation for downstream analysis.

### Batch Effect Correction and Data Integration

Because experiments are done at different times or sites with varying protocols, **batch effects** can obscure biological signals. A major focus of recent work has been using deep generative models to integrate multiple single-cell datasets and remove batch or sample-specific noise. Many methods adopt a **domain adaptation** strategy, where an autoencoder-based model learns a **shared latent representation** that is invariant to batch labels. Typically, an encoder compresses each cell’s expression into a latent vector, and a decoder reconstructs the data; an adversarial **discriminator** network is trained simultaneously to try to predict the batch origin of each cell from its latent code. The encoder is optimized to fool the discriminator, thus removing batch-specific information from the latent representation. For example, **iMAP** (2021) combines an autoencoder with two generative decoders and a GAN discriminator to produce batch-insensitive cell embeddings. When applied to tumor microenvironment datasets from two platforms, iMAP effectively merged the datasets, leveraging the strengths of each and enabling biological discovery (identifying cell–cell interactions) that was validated by independent methods. Another method, **DAVAE (Domain-Adversarial VAE, 2021)**, uses a **gradient reversal layer** in a VAE to remove batch differences – essentially, the encoder learns an encoding that a domain (batch) classifier cannot distinguish between batches. This idea was extended in **scDGN**, a supervised domain generalization network that maximizes cell type classification accuracy while minimizing batch differences, to ensure that latent features are both biologically informative and batch-invariant. Adversarial approaches have also been used to eliminate multiple confounders at once: **AD-AE** (2020) introduced an adversarial arm to predict known “confounders” (batch, age, etc.) from the latent space; by alternating training steps, the model learns to remove any features that let the adversary predict the confounders, yielding a cleaned embedding. Variations on this theme include **scGAN** (2021), which combines a VAE with a discriminator to align batches, and **trVAE** (2019) which uses a conditional VAE with maximum mean discrepancy (MMD) loss to encourage overlap of batch distributions in latent space. An interesting innovation by Pang & Tegnér (2021) applied a **Transformer (BERT)** encoder to gene expression profiles, coupled with adversarial training for batch alignment. This attention-based model (the BERT architecture) encoded cells in a latent space and a GAN removed batch differences, illustrating the versatility of DL architectures (see Model Architectures section). Besides GANs, **distribution-matching** approaches have been used: for example, **BERMUDA** (2019) trains separate AEs on each batch and adds a loss term to match the latent spaces of the two batches, aligning their distributions. **SCALEX** (2020) introduced domain-specific normalization layers within a VAE to account for batch-specific variance directly. Overall, deep integration methods allow large multi-sample or multi-condition atlases to be harmonized; for instance, the widely used **scVI** framework (discussed below) learns a probabilistic latent space that naturally accounts for batch and donor effects and has been applied to integrate massive human cell atlases.

### Dimensionality Reduction and Clustering

To interpret single-cell data, high-dimensional gene expression profiles must be distilled into lower-dimensional representations or clusters of similar cells. Traditional methods like PCA or t-SNE/UMAP are unsupervised and not tailored to single-cell quirks (zero inflation, nonlinear gene interactions). Deep autoencoders and related models have therefore been embraced for single-cell **feature extraction** and **clustering**. A basic **autoencoder (AE)** can learn a nonlinear compression of the data (the encoder’s latent space) that preserves most information needed to reconstruct the input. Early work showed that a deep autoencoder could outperform PCA in capturing cell subpopulation structure. Many variants have since been proposed to enhance clustering in the latent space. **SAUCIE** (2018) is an autoencoder that adds a **clustering-specific regularization**: it penalizes the information content (dimension) of the latent representation and explicitly encourages clusters in latent space by a distance-based loss. This drove the network to organize latent codes into well-separated groups, improving unsupervised cell clustering. **scDeepCluster** and related methods incorporate a clustering objective (e.g. k-means or fuzzy k-means) directly into the AE training, aligning latent features with cluster assignments iteratively (a form of **deep embedded clustering**). For example, **scziDesk** (2020) performs weighted soft k-means on the latent space during training, and **DEC** variants use an auxiliary target distribution to refine clusters. These approaches can automatically find cell types while learning the embedding. **Variational Autoencoders (VAEs)** have been especially influential. **scVI** (2018) introduced a VAE tailored to scRNA-seq count data, using a ZINB likelihood to model gene counts and incorporating cell-specific size factors. By sharing information across similar cells and genes, scVI’s latent space captures biological structure while accounting for technical noise and batch effects. Cells of the same type cluster together in this latent space, so simple algorithms on the latent (like kNN+community detection) yield accurate clusters. Many follow-ups to scVI have appeared: **scVAE** added a mixture-of-Gaussians prior to encourage clustering in the VAE latent space, and **scPhere** (2021) even embedded cells in *hyperbolic* latent space (using a hyperbolic VAE) to better separate cell populations in a hierarchy. Graph-based deep models also emerged: **GraphSCC** (2021) combines a **graph convolutional network (GCN)** on a cell similarity graph with a denoising autoencoder, jointly refining the cell graph and latent features for clustering. Another, **scGNN** (2021), integrates multiple autoencoders (including a graph autoencoder) to perform an entire pipeline from imputation to clustering. These graph-augmented approaches leverage cell–cell relationships to enhance clustering of rarer or subtle cell states. Table 1 lists a few of these prominent methods. In practice, deep embeddings have become popular in pipelines – for example, the workflow of **Scanpy** (a common toolkit) allows using scVI or others to generate an embedding before clustering, which often improves results over PCA. It’s worth noting that interpretability can be a challenge: one extension of scVI, called **LDVAE**, replaced the nonlinear decoder with a single-layer (linear) decoder to make the relationship between latent features and genes more interpretable. This helps assign biological meaning to latent dimensions at some cost in clustering performance.

### Trajectory Inference and Dynamics

**Trajectory inference** (pseudotime ordering and lineage branching) has mostly been addressed by advanced statistical methods (e.g. diffusion pseudotime, graph-based approaches). However, a few deep learning innovations have tackled the **temporal dynamics** of single-cell data. **RNA velocity** analysis, which uses spliced/unspliced mRNA to infer directionality of cell state changes, inspired deep models like **VeloAE** (2020) and **DeepCycle** (2020). VeloAE is a variational autoencoder that incorporates RNA velocity vectors, learning a latent representation of underlying transcriptional dynamics. By modeling the splicing kinetics with a generative model, it captures developmental trajectories more robustly than linear methods. Similarly, **DeepCycle** uses a cyclic architecture to map cell-cycle progression. These were specialized advances and were not covered in some surveys due to their unique input requirements. More broadly, some researchers have applied recurrent neural networks (RNNs) or **temporal autoencoders** to pseudotime-ordered data to learn smoother trajectories, but these are less common than statistical methods. A related area is **generative modeling of perturbations**: **scGen** (2019) is a variational autoencoder that learns to encode not just cell identity but also a vector for perturbation (like a treatment vs control difference), enabling simulation of how a given cell’s gene expression would change under a perturbation. This has been used to predict drug responses at single-cell resolution. Overall, while deep learning hasn’t dominated trajectory inference the way it has clustering or denoising, it’s starting to contribute, especially by combining transcriptional dynamics with deep generative frameworks (e.g. **veloVI** in 2022 applied scVI-like modeling to RNA velocity).

### Cell Type Annotation and Classification

Identifying cell types from single-cell data can be approached as a **supervised learning** problem. Traditional pipelines often use marker genes or correlate to reference profiles, but in recent years many ML/DL classifiers have been developed to automate this step using reference datasets. A straightforward approach is training a neural network to predict cell type from the gene expression vector. **ACTINN** (2019) is one such example – a simple fully-connected deep neural network that was trained on known cell types and achieved high accuracy on labeling new cells. Random forest classifiers (e.g. **SingleCellNet, 2019**) and support vector machines were also tried; these are effective for small panels of marker genes but less so for high-dimensional data. Deep learning offers the ability to handle the full gene set and complex patterns. **scANVI** (2021) extended the scVI VAE to a **semi-supervised model**: it trains on a mixture of labeled and unlabeled cells, using the labeled cells to inform latent dimensions that correspond to known cell types. In practice, scANVI can take an atlas with some annotated cells and accurately label new cells, even detecting when a cell doesn’t match any known type (by its uncertainty). Another notable approach is **ItClust** (2020), which pre-trains a denoising autoencoder on a well-annotated reference dataset and then fine-tunes it on a new dataset for label transfer. By leveraging a reference latent space, ItClust improves transfer to a target domain with limited or no labels, and was shown to outperform classic correlation-based mapping in cross-dataset cell type assignments. **scNym** (2021) introduced semi-supervised training with MixMatch (a data augmentation technique from computer vision) to single-cell data, demonstrating that using unlabeled data in training improved classification of novel datasets. Some methods integrate **graph neural networks** for annotation: **scDeepSort** (2021) builds a cell–gene bipartite graph and uses a graph convolutional network to predict cell types, effectively leveraging gene–cell co-occurrence patterns to refine predictions. Similarly, **CellTypist** (2021) and others use logistic regression or simple DNNs on very large training sets (like >1M cells) to create fast, reusable classifiers. The challenge in cell type annotation is often *interpretability* and *transferability*. In 2023, a model called **TOSICA** (Transformer for One-Stop Interpretable Cell Annotation) applied a transformer architecture with multi-head self-attention to this task, showing that attention weights could highlight hallmark genes for each cell type, thus providing biological insight while classifying. We discuss transformers more below, but TOSICA exemplifies how attention mechanisms can make DL models more transparent in genomics. In practice, many analysis workflows now combine an automated classifier with manual curation: e.g. using a DL model’s predictions as a starting point and then adjusting based on known biology. As reference atlases grow, we expect **“pre-trained” models** (analagous to ImageNet models in vision) to become standard for cell type annotation. Indeed, the idea of a **single-cell “foundation model”** is emerging – for example, a recent preprint described **scGPT**, a generative transformer model trained on millions of cells that can then be fine-tuned for specific tasks.

### Summary of Key Methods in the Pipeline

The table below summarizes key ML/DL methods for scRNA-seq analysis, illustrating the breadth of the pipeline that has been enhanced by these models:

**Table 1. Representative ML/DL Methods for scRNA-seq Data Analysis (2019–2024).**

| **Task**                   | **Method (Year)**      | **Approach**                            | **Key Details / Use Case**                                                                                                                                |
| -------------------------- | ---------------------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Denoising / Imputation** | *DeepImpute* (2019)    | DNN regression (feed-forward)           | Impute dropouts using correlated genes; train on subsets of genes for efficiency. Improved accuracy over KNN methods.                                     |
|                            | *DCA* (2018)           | Autoencoder + ZINB loss                 | Variational autoencoder reconstructs counts using ZINB model of noise. Output mean gives denoised data.                                                   |
|                            | *scIGAN* (2020)        | Generative Adversarial Network (GAN)    | Generates synthetic cells to model and impute missing expression. Alleviates dropout by learning data distribution.                                       |
|                            | *SAVER-X* (2019)       | Autoencoder + transfer learning         | Pretrains on large public data (mouse/human) to learn gene relations, then denoises target data. Cross-species knowledge improves human data imputation.  |
| **Batch Integration**      | *scVI* (2018)          | Variational Autoencoder (probabilistic) | Learns latent space accounting for batch effects; yields integrated embeddings for clustering. Widely used for atlas integration.                         |
|                            | *iMAP* (2021)          | Adversarial Autoencoder (GAN)           | Encoder + dual decoders for expression; adversarially removes batch info. Integrates cross-platform datasets, enabling joint analysis.                    |
|                            | *trVAE* (2019)         | Conditional VAE + MMD                   | Uses cell-type labels as conditioning and MMD regularization to align batch distributions in latent space. Effective for known cell types across batches. |
|                            | *SCALEX* (2020)        | VAE with batch-normalization layers     | Domain-specific BN in decoder to model batch-specific variance. Scales to large datasets (single-cell multimodal).                                        |
| **Clustering / DR**        | *SAUCIE* (2018)        | Autoencoder with clustering loss        | Regularizes latent dimension and inter-cluster distances. Unsupervised clustering of cells; used for visualization and batch correction.                  |
|                            | *scVI* (again) (2018)  | VAE (ZINB model)                        | (See above.) Latent space often used with kNN+Louvain to define clusters; accounts for technical noise.                                                   |
|                            | *scDeepCluster* (2019) | Autoencoder + iterative clustering      | Jointly learns latent features and cluster assignments (self-training). Discovers cell types de novo.                                                     |
|                            | *scGNN* (2021)         | Graph AE + multiple AEs                 | Comprehensive framework: graph AE integrates cell–cell graph with gene AE. Performs imputation, clustering in one model.                                  |
|                            | *scPhere* (2021)       | Variational AE (hyperbolic latent)      | Embeds cells in a hyperbolic space for hierarchical clustering (captures continuous differentiation trajectories).                                        |
| **Trajectory Inference**   | *VeloAE* (2020)        | Variational AE for RNA velocity         | Learns latent dynamics from spliced/unspliced counts. Models developmental trajectories with probabilistic time.                                          |
|                            | *DeepCycle* (2020)     | Autoencoder with cyclic constraints     | Learns a circular manifold corresponding to cell cycle progression. Identifies cell-cycle phase transitions.                                              |
|                            | *scGen* (2019)         | VAE (perturbation vector arithmetic)    | Learns latent space of cells; encodes perturbation effect as a vector. Can simulate drug effect on any cell.                                              |
| **Cell Type Annotation**   | *ACTINN* (2019)        | DNN classifier (fully-connected)        | Trained on known cell types to predict labels for new cells. Early demonstration of NN for single-cell type ID.                                           |
|                            | *scANVI* (2021)        | Semi-supervised VAE                     | Extends scVI: uses partial labels to inform latent space and classify cells. Handles “unlabeled” novel cell types by uncertainty.                         |
|                            | *ItClust* (2020)       | Transfer learning with Denoising AE     | Pretrain on labeled reference, fine-tune on target. Achieves accurate label transfer between datasets.                                                    |
|                            | *scNym* (2021)         | Semi-supervised (MixMatch) DNN          | Augments data and uses consistency loss to learn from unlabeled cells. Robust to domain shift in new experiments.                                         |
|                            | *scDeepSort* (2021)    | Graph Neural Network (GNN) classifier   | Builds a cell–gene graph, uses GCN to predict cell types. Leverages gene connectivity for higher accuracy in cell ID.                                     |
|                            | *TOSICA* (2023)        | Transformer (attention-based)           | Multi-head attention model for cell types. Provides interpretability by highlighting marker genes (via attention weights) while classifying.              |

*Table 1:* **Key single-cell analysis methods using ML/DL.** DR = dimensionality reduction. AE = autoencoder. VAE = variational AE. GNN = graph neural network. ZINB = zero-inflated negative binomial. MMD = maximum mean discrepancy. (References indicated in brackets.)

As shown, deep learning has permeated every analysis stage — from cleaning the data to identifying cell states — often providing more accurate or scalable solutions than previous methods. Next, we focus on specialized applications in immune repertoire analysis and spatial transcriptomics, followed by a discussion of the underlying model architectures that power these advances.

## T-Cell and B-Cell Repertoire Analysis with Machine Learning

The adaptive immune system relies on enormous diversity in T-cell receptors (TCRs) and B-cell receptors (BCRs) to recognize antigens. High-throughput sequencing of TCR and BCR repertoires (including single-cell approaches that pair receptor sequences with cellular phenotype) has led to new analysis challenges. In the past five years, researchers have applied ML and DL to **repertoire analysis** – uncovering patterns in the sequences and clonal abundances of TCRs/BCRs – to answer immunological and clinical questions. Below we discuss advances for T cells and B cells, noting that some methods are common to both (e.g. sequence-based modeling techniques). **Table 2** at the end of this section summarizes key tools.

### T-Cell Receptor (TCR) Repertoire Analysis

Each T cell has a unique TCR (typically consisting of an α and β chain) generated by V(D)J recombination. Collectively, the set of TCR sequences in an individual (the repertoire) encodes a wealth of information about immune status and history. Traditional TCR repertoire analyses involved calculating diversity metrics, examining clonal expansions, or motif-based clustering of similar sequences (e.g. using tools like GLIPH). New ML/DL methods go further by **learning sequence patterns** and linking TCRs to functional outcomes.

One major application is **repertoire classification**: using TCR sequences to distinguish disease vs. healthy or different conditions. **DeepTCR** (2021) was a seminal deep learning framework that tackled this by combining unsupervised and supervised learning. It uses a variational autoencoder to embed TCR sequences (learning a “concept” space of TCR motifs/features), and then uses those features in a classifier to solve tasks like identifying patients with a certain disease from their TCR repertoire. Essentially, DeepTCR learns a latent representation of TCR sequences (capturing gene segment usage, CDR3 motifs, etc.) without human feature engineering, and those representations improved classification performance on tasks such as cancer vs. healthy based on TCRβ sequences. Another model, **DeepRC** (2021), introduced an attention-based neural network to classify repertoires. DeepRC encodes each TCR sequence in a repertoire as a vector (using a convolution or recurrent network) and then applies a trainable **attention mechanism** to weight each sequence’s contribution when predicting an outcome. This is analogous to treating the repertoire as a “document” of sequences and using attention (similar to NLP models) to find which sequences are most relevant for the classification (e.g. specific clonotypes expanded in disease). These methods can pick up subtle shifts in TCR frequencies or sequence motifs that classical diversity metrics might miss.

Another active area is **TCR–antigen specificity prediction**. Here, the goal is to predict if a given TCR (sequence) binds a particular peptide–MHC (antigen). This is essentially a binary or multi-class classification at the sequence level, often tackled with deep learning due to the complex sequence interactions. Models in 2020–2021 began to leverage techniques from natural language processing (treating amino acid sequences as “text”). For example, one study used a sequence-to-sequence translation model to pair TCRs with their target epitopes, and another used a Siamese network to embed TCRs and peptides in the same space. A notable approach by Lu *et al.* (2021) used a deep learning model with an **attention mechanism** to predict TCR–antigen binding, published in *Nature Machine Intelligence*. Around the same time, Sidhom *et al.* (2021) showed that incorporating *biologically informed* sequence context (such as motifs that correspond to structural contacts) can improve predictions. In general, while early TCR–epitope prediction efforts (pre-2020) found that simpler models sometimes rivaled deep nets (likely due to limited training data), recent larger datasets (millions of TCRs with known specificities) are enabling transformers and convolutional networks to start outperforming older methods in certain cases. For instance, a 2022 study introduced **BERT-TCR**, adapting the BERT transformer model to TCR sequence data. **BertTCR** (2024) pre-trains a BERT-like model on large TCR sequence corpora (learning “language” features of TCRs) and then fine-tunes it to predict **cancer-associated vs. normal** repertoires. This model was able to capture subtle sequence motifs that correlate with cancer status, effectively distinguishing cancer patients from controls based solely on TCR sequences. The transformer’s self-attention allowed the model to consider interactions between distant amino acids in the TCR sequence, which is important for TCR antigen-binding conformation.

Beyond classification and specificity, ML is used for **clonotype and diversity analysis**. Clonotypes are groups of T cells with identical TCR sequences, often expanded from a single progenitor. Researchers have applied unsupervised learning to cluster TCR sequences by similarity, hoping to discover antigen-driven “public” motifs. While algorithms like *TCRdist* (distance metric) and *GLIPH2* (2021) use heuristic rules to cluster TCRs, deep learning can learn the optimal notion of similarity from data. For instance, variational autoencoders have been used to embed TCR sequences into a continuous space where distance correlates with sequence and possibly antigen similarity. In one approach, a VAE was trained on a large set of TCR sequences from healthy individuals to model the generation process (essentially learning the “baseline” repertoire diversity), and deviations in latent space were used to flag unusual clonal expansions. There are also graph-based techniques: *TCRGP* (2021) represented TCR repertoires as graphs and used graph kernels to classify CMV infection status. Meanwhile, combination models have appeared: a recent 2023 *Science* paper by Zaslavsky *et al.* introduced **Mal-ID (Machine Learning for Immunological Diagnosis)**, which integrates TCR *and* BCR sequence features for disease diagnosis. Mal-ID trained on a cohort of 593 individuals spanning healthy, autoimmune diseases, viral infections, and vaccine responses. It computed multiple feature representations for each person’s TCR and BCR repertoire (such as V/J gene usage frequencies, clonality metrics, k-mer frequencies, and learned sequence embeddings) and combined them in an ensemble model. The resulting classifier could distinguish controls from disease patients and even differentiate between diseases (e.g. rheumatoid arthritis vs. COVID-19 infection vs. recent influenza vaccine) based solely on immune receptor sequences. Notably, Mal-ID achieved broad diagnostic accuracy, suggesting that “immune signatures” captured by ML from repertoire sequencing can serve as biomarkers. This study highlights the power of combining domain knowledge (hand-crafted features like gene usage biases) with deep learning (sequence embeddings). It also underscores that TCR analysis is increasingly data-driven: with enough sequencing data, models can discern patterns (public clones, convergent sequence features, etc.) that correlate with specific immune states.

In summary, ML/DL in TCR repertoire analysis has enabled: (1) **Unsupervised pattern discovery** – finding motifs or reduced dimensions representing the repertoire; (2) **Supervised classification** – predicting disease states or antigen specificities from sequences; and (3) **Integration with other data** – for example linking TCR sequence patterns with single-cell gene expression (to see if certain clonotypes correspond to particular T cell phenotypes like exhausted T cells). Many of these tools (DeepTCR, GLIPH2, TCRdist, ERGO, ImRex, etc.) have been benchmarked in the AIRR community, and while no single model dominates all tasks, the trend is towards more complex models as data grows. A current challenge is interpretability: unlike motif-based methods, deep models can be black boxes. But techniques like attention and post-hoc motif extraction are being used to make sense of what sequence features drive a model’s predictions, aligning them with biochemical knowledge.

### B-Cell Receptor (BCR) Repertoire Analysis

BCR (antibody) repertoires pose similar challenges with some unique twists: B cells undergo somatic hypermutation and class-switching upon antigen exposure, so repertoires contain clonal lineages (groups of sequences related by mutations) that reflect an evolutionary process. Analysis goals include identifying antigen-specific clones (e.g. antibodies to a virus), tracking clonal expansion in response to vaccines, and diagnosing diseases (like evaluating abnormal BCR clones in autoimmune disorders or B-cell cancers).

Historically, BCR analysis relied on clustering sequences by V gene usage and CDR3 similarity to define clonotypes, then building lineage trees. Machine learning is now helping in **two main ways**: improving BCR sequence classification (e.g. predicting B cell subsets or antigen binding from sequence) and combining repertoire features for clinical insights.

A notable recent advance is using deep learning to **predict B-cell subset or maturation state from the BCR sequence alone**. Normally, determining a B cell’s subset (naïve vs memory vs plasma cell, etc.) requires phenotypic markers or single-cell transcriptomes. **BCR-SORT** (2024) showed that the patterns of somatic mutations in the BCR sequence carry enough information to predict the cell’s subset using DL. BCR-SORT uses a deep neural network that was trained on BCR sequences labeled with the B cell’s subset (obtained from paired single-cell data). It learns sequence features associated with naive B cells (few mutations, certain gene usage) vs. memory B cells (mutated, class-switched) etc.. The model leverages “activation and maturation signatures encoded within BCR sequences” to classify cells. Impressively, BCR-SORT’s predictions help reconstruct clonal lineage trees more accurately, as it can tell if a sequence likely came from a naive or memory cell and group accordingly. When applied to real data from COVID-19 vaccine recipients, BCR-SORT uncovered differences between individuals in how their B cell lineages evolved to target the SARS-CoV-2 Omicron variant. This demonstrates that **deep learning can infer functional immunology properties (like memory vs naive status) directly from receptor sequences**, a task previously thought to require lab assays.

Deep learning has also been applied to **antibody-antigen binding** prediction, analogous to TCR. Because there is a wealth of data on antibodies binding specific targets (e.g. from phage display or structural databases), researchers have trained models to predict if a given BCR (often just the heavy chain or the paired heavy/light) will bind a particular antigen or virus. Some approaches treat this as a binary classification per antigen (similar to TCR-epitope prediction), using CNNs or LSTMs on the sequence. Others generate embeddings for BCRs and known antigen-binding sequences to find similarities. A challenge is that antibody binding is very dependent on 3D structure; thus sequence-based models have limitations. Still, language-model style approaches are promising: e.g., a 2021 bioRxiv by Ruffolo *et al.* introduced **Antibody Specificity Predictor** using a transformer trained on hundreds of thousands of antibody sequences labeled by antigen, achieving decent accuracy in matching antibodies to their antigen family.

On the repertoire level, **disease diagnostics** using BCR sequences has seen progress. For example, in autoimmune diseases like lupus or celiac disease, characteristic biases in BCR repertoires (such as enrichment of certain heavy-chain genes or elevated somatic hypermutation levels) can serve as biomarkers. Traditional analysis would measure these one by one, but ML can combine many features. The Mal-ID framework mentioned above included BCR sequence features in its multi-modal model and found that including BCR data improved discrimination of autoimmune diseases. Another study in *Science* (2023) by Emerson *et al.* used a random forest on BCR/TCR k-mer frequencies to predict recent infection with specific viruses. And in 2019, Yaari and Kleinstein’s groups showed that simple ML classifiers on BCR repertoire features could distinguish patients with an infection from vaccinated or healthy individuals. These studies indicate that even without explicit antigen specificity, the **compositional shifts** in BCR repertoires (clonal expansions, hypermutation distribution, isotype usage) contain signals that ML algorithms can pick up for diagnostic purposes.

In terms of unsupervised analysis, deep generative models have been used to characterize the baseline diversity of BCR repertoires. For instance, a **deep generative model** of BCR sequences can learn the probability distribution of naive BCR generation (similar to how language models learn word distributions). Davidsen *et al.* (2019) did this with a VAE for TCRs, and similar approaches apply to BCR. Such models are useful for detecting antigen selection: if an observed repertoire deviates from the generative model’s expectations (i.e. certain sequences are overrepresented beyond what random generation would predict), it indicates antigen-driven expansion. This intersects with statistical methods but with a more flexible model of sequence space. Moreover, VAEs can generate *new* antibody sequences, which has implications for therapeutic design (though that’s slightly beyond analysis into design).

One fascinating development is integrating **BCR sequence data with single-cell transcriptomes** to understand B cell function. Single-cell multi-omics technologies (e.g. 10x Genomics V(D)J sequencing along with gene expression in the same cell) allow linking the BCR sequence to the cell’s state. ML can then relate the two: for example, correlating a BCR’s predicted binding specificity (perhaps via a model) with the cell’s transcriptional profile (plasma cell vs memory B cell). This can reveal if, say, all B cells specific for a certain antigen preferentially become a particular subtype.

To sum up, ML in BCR analysis is enabling: automated identification of B cell states from sequence, prediction of antigen targets (or at least grouping of similar antibodies), and improved use of repertoire data for clinical insights. The field is slightly behind TCR analysis in terms of number of DL papers, possibly because BCRs have more complex genetics (paired heavy/light, hypermutations) and data sharing was less standardized until recently. But with efforts like the Observed Antibody Space (OAS) database and improvements in single-cell BCR sequencing, we’re seeing rapid progress.

**Table 2. ML/DL Methods for TCR/BCR Repertoire Analysis.**

| **Task**                          | **Method / Tool (Year)**            | **Approach**                                                 | **Use Case / Findings**                                                                                                                                                                                        |
| --------------------------------- | ----------------------------------- | ------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **TCR repertoire classification** | *DeepTCR* (2021)                    | VAE + DNN classifier                                         | Learns unsupervised TCR sequence features (via VAE), uses them to classify conditions. Applied to cancer vs. normal TCRβ repertoires – identified sequence patterns linked to tumor response.                  |
|                                   | *DeepRC* (2021)                     | Attention-based deep ensemble                                | Encodes each TCR in repertoire; attention mechanism weighs their importance. Used to classify disease from bulk TCR repertoires, highlighting specific clonotypes driving classification.                      |
|                                   | *Zaslavsky *et al.* (Mal-ID, 2025)* | Ensemble (multiple feature representations + ML classifiers) | Integrated six types of TCR/BCR sequence features to predict disease. Distinguished autoimmune diseases, viral infections, and vaccine responses from repertoire data alone.                                   |
| **TCR–peptide specificity**       | *ATTEMPT* (2021)                    | Attention-based sequence pair model                          | Uses paired encoder for TCR and peptide, with attention to predict binding. Improved accuracy on some benchmark datasets of TCR–pMHC binding.                                                                  |
|                                   | *NetTCR-2.0* (2020)                 | CNN on TCR sequences                                         | Predicts if TCR (CDR3β) recognizes a given peptide. Benchmark study showed high ROC-AUC on some viral targets.                                                                                                 |
|                                   | *TITAN* (2022)                      | Bimodal neural network                                       | Processes TCR α and β chains and peptide sequence jointly. Incorporates contact site predictions. Outperforms earlier models in specific tasks.                                                                |
| **BCR subset prediction**         | *BCR-SORT* (2024)                   | Deep neural network (multi-layer perceptron)                 | Predicts B cell subset (naïve, memory, etc.) from BCR sequence. Learned mutation patterns that correlate with activation state; validated by reconstructing known lineage relationships.                       |
| **Antibody specificity**          | *Ig-VAE* (2020)                     | VAE for antibody sequences                                   | Generates and evaluates antibody CDR sequences. Can be used to propose novel antibodies or to cluster antibodies by learned “antigen space”.                                                                   |
|                                   | *AbLang (2021)*                     | Language model (Transformer)                                 | Pretrained on millions of BCR sequences to learn antibody “language”. Used for zero-shot prediction of binding: e.g. find which sequences in repertoire likely target a given antigen by comparing embeddings. |
|                                   | *Briney *et al.* (2019)*            | K-mer SVM                                                    | (Classic ML) Used k-mer frequencies to classify antigen-specific vs non-specific BCRs for particular antigens. ML outperformed random selection, but deep nets began to surpass SVM with more data by 2022.    |
| **Clinical repertoire signature** | *Emerson *et al.* (2017)*           | Statistical + ML (random forest)                             | One of the first to use TCRβ repertoires to predict CMV status in humans. Not deep learning, but set stage for later DL by proving repertoire could predict infection.                                         |
|                                   | *ImmuneML platform (2022)*          | ML pipeline toolbox                                          | Provided a unified framework to apply various ML/DL (random forest, CNN, RNN, etc.) on repertoire datasets. Aids in benchmarking multiple models for tasks like disease classification.                        |
|                                   | *Disease-specific BCR sig.* (2021)  | Random forest on repertoire features                         | Detected SLE (lupus) vs. healthy by feature selection and ML on BCR sequence descriptors. Showed that even without known antigen, repertoire shifts serve as biomarkers.                                       |

*Table 2:* **Key ML/DL methods in TCR/BCR repertoire analysis.** This includes sequence-level models (for predicting specificity or subset) and repertoire-level models (for classification of immune status). The field is evolving quickly; methods marked with (\*) are notable but slightly older (pre-2019) and included for context. Mal-ID is highlighted as a recent comprehensive model leveraging both TCR and BCR data.

In summary, ML and DL have started to unlock the informational content of immune repertoires. By learning from large cohorts, these models can detect “immune fingerprints” of diseases or responses that were previously indiscernible. As more data becomes available (e.g. from projects like the Human Immunome Project), we anticipate even more accurate and interpretable models, possibly integrated with structural predictions (AlphaFold2 for TCR–pMHC or antibody–antigen complexes) to truly bridge sequence with function. The combination of single-cell data (linking TCR/BCR to cell phenotype) with deep learning models is a particularly exciting avenue, likely to yield personalized immunoprofiling tools in the near future.

## Spatial Transcriptomics Analysis with Deep Learning

Spatial transcriptomics (ST) technologies (such as 10x Visium, Slide-seq, MERFISH, etc.) profile gene expression **in situ**, preserving each cell’s or spot’s location in a tissue. Since 2019, ST has become an extension of single-cell RNA-seq, adding a layer of spatial context (often along with histology images). The data are inherently **multimodal**: we have gene expression matrices, spatial coordinates, and often high-resolution tissue images. Traditional analysis methods (clustering spots by expression, detecting spatial variable genes with statistical tests) can miss complex spatial patterns. Deep learning, with its strength in image analysis and pattern recognition, has been rapidly adopted to tackle various tasks in spatial transcriptomics. These tasks include: (1) **Spatial domain (region) identification** – clustering cells/spots into tissue substructures using both expression and spatial information; (2) **Integration of histology and transcriptomics** – using convolutional neural networks (CNNs) on tissue images to enhance or predict gene expression patterns; (3) **Deconvolution and cell type mapping** – inferring which cell types are present at each location, often by integrating single-cell reference data; (4) **Spatially variable gene detection** – identifying genes with spatially patterned expression; and (5) **Cell–cell interaction inference** in spatial context. We will highlight how DL/ML has been applied in these areas. **Table 3** summarizes representative methods.

### Integrating Histology Images and Gene Expression

A unique aspect of many spatial transcriptomic datasets (e.g. Visium) is the availability of a *paired tissue image* (usually an H\&E stained slide) for the same section where gene expression was measured. This image contains rich morphological information – cell shapes, tissue architecture, etc. – which can be correlated with gene expression. Deep learning has been pivotal in integrating these two modalities.

One straightforward application is using CNNs to **predict gene expression from histology images**. This addresses a practical challenge: ST assays are expensive and not always feasible at large scale, but digital histology slides are routine. If one can train a model on samples where both ST and image data are available, the model could then predict spatial expression in new samples from image alone. For example, **ST-Net** (2020) was an early deep learning algorithm that took in image patches and output gene expression profiles (or low-dimensional embeddings of them), effectively learning the correspondence between visual features and transcriptomic patterns. More recently, **BrST-Net** (2023) tackled this in breast cancer tissue: they trained several deep architectures (ResNet, Inception-v3, EfficientNet, even Vision Transformers) to **predict the expression of 250 genes from histology images**. Their best model identified hundreds of genes whose expression could be reliably inferred from H\&E images (correlation >0.5 for 24 genes, and positive correlation for 237 genes in their test set). This significantly outperformed earlier studies that could predict only \~100 genes with much lower correlations. The implication is that for many genes, especially those related to morphology (e.g. collagen genes, immune cell markers), the histological image contains enough information for a DL model to guess the gene’s expression level. Such models can be used to generate *virtual gene expression maps* on thousands of archived pathology slides, enabling pseudo-spatial-transcriptomics at scale. For example, one could map out the immune infiltration or tumor gene expression in a whole slide image using a model trained on a smaller ST dataset.

Beyond predicting gene expression, integrating images can **improve clustering and spatial domain detection**. An influential method is **stLearn** (2020), which proposed a combined analysis: it computed image-derived features for each ST spot (like nucleus density, texture) and used those to augment the gene expression in a graph-based clustering. stLearn introduced “Spatial Morphological Epigenetic” (SME) normalization – essentially smoothing gene expression based on nearby spots with similar histology – to reduce technical noise. Pham *et al.* showed that including histology in this way produced more contiguous and anatomically meaningful clusters, outperforming purely expression-based clustering on both mouse brain and human brain data. In their benchmarks, stLearn’s morphology-aware approach outdid Seurat and other methods in delineating known anatomical regions. This underscores a key point: **morphological context helps resolve ambiguities** where expression alone might mix cells from distinct regions that have similar profiles. By incorporating image features, deep learning models can ensure that identified domains make sense in light of tissue structure.

Several other methods follow this multi-modal strategy. **SpaCell** (2019) used a two-branch deep network: one branch processes image patches with a CNN, another processes the expression vector, then merges them to cluster spatial spots. **SpaGCN** (2021) (discussed in the next subsection) also integrates image features into its graph neural network. **GIST** (2022) is a method that explicitly aligns histology and gene expression embeddings using a deep model. **STAIG** (2023, Nature Methods) proposed a graph-contrastive learning approach to integrate gene, coordinate, and image data: it uses a GCN where node features are gene expression and additional image features, and a contrastive loss to ensure the integrated representation captures both modalities well.

A remarkable outcome of these image/gene integration efforts is that in some cases, the **image alone can be used to infer molecular subtypes**. For instance, a Nature Cancer (2025) study on brain tumors introduced **NePSTA**, which uses spatial transcriptomics with graph neural networks to do automated histopathological diagnosis. They reported that NePSTA could predict molecular tumor subclasses (which normally require DNA/RNA assays) directly from a single tissue section’s ST data plus image. The GNN effectively combined image features and expression to classify tumor type with high accuracy, and even recapitulated results of immunohistochemistry and mutational analysis from just the one assay. This indicates a future where pathology and genomics blend: a deep model examines a slide and outputs both histological features and underlying gene expression patterns (or diagnoses that typically come from sequencing).

### Spatial Domain Identification (Clustering) with Graph Neural Networks

Spatial transcriptomic data often need to be segmented into “domains” – regions of the tissue with distinct expression profiles (for example, layers of brain cortex, tumor vs. stroma regions, etc.). Unlike scRNA-seq clustering, here physical proximity and image context matter: adjacent spots likely belong to the same domain. Graph-based methods are natural for this, since one can create a graph of spots (nodes) connected to their neighbors.

**SpaGCN** (Graph Convolutional Network for spatial analysis) is a leading method in this category. It constructs a weighted graph of the spatial locations and feeds gene expression (and optionally image features) into a graph convolutional network to cluster the spots. Each spot is a node connected to nearby spots; edges have weights based on both distance and histology similarity (e.g. color intensity difference). The GCN propagates information between neighboring spots while learning low-dimensional embeddings. SpaGCN then performs **clustering in the output embedding space**, yielding spatially coherent domains. In training, SpaGCN optimizes a loss function that encourages cluster assignments to be confident and neighborhoods to be smooth, akin to a **clustering-friendly loss (KL divergence between soft cluster assignment and a target distribution)**. Hu *et al.* demonstrated that SpaGCN accurately identified domains matching expert annotations in human brain and pancreatic cancer tissues. Importantly, it outperformed earlier methods like stLearn, BayesSpace, and non-spatial clustering in consistency and resolving fine substructures. By leveraging both gene and image data, SpaGCN achieved a nuanced segmentation of tissues that aligned with known histological regions (e.g., layering in cortex, tumor microenvironment niches). SpaGCN is widely cited and has become a reference method for graph-based spatial clustering.

Following SpaGCN, other GNN models emerged: **STAGATE** (2022) uses a Graph **Attention** Network (GAT) to integrate each spot with its neighbors’ information. The attention mechanism can learn different weights for different neighboring spots, which might handle irregular spatial layouts better. STAGATE showed improved performance on some Visium datasets, especially in separating adjacent but distinct cell populations (the GAT can effectively “cut” between domains if there is a sharp expression boundary). **SEDR** (2021) combined a standard autoencoder for gene expression with a **variational graph autoencoder (VGAE)** for the spatial graph, linking them to produce a joint embedding. SEDR’s latent representation thus captures both expression similarity and spatial adjacency, yielding clusters that are spatially continuous without needing image data. It was noted that while SEDR didn’t use histology, methods like SpaCell and stLearn that did include image features had advantages in heterogeneous tissues.

Another line of work involves identifying **spatially variable genes (SVGs)** – genes whose expression varies in patterned ways across the tissue. Traditional methods (e.g. Moran’s I test, Geary’s C) exist, but ML can assist, for example by using spatially-aware feature selection. Some deep models like SpaGCN simultaneously identify SVGs as part of finding domains (the clusters define high vs low expression regions for certain genes). There are also specific methods: **SpatialDE** (2018, non-DL) was statistical, but newer approaches use Gaussian Processes or even a form of convolutional network treating coordinates as inputs to detect genes with spatial gradients.

### Cell-Type Deconvolution and Cross-Modality Mapping

Spatial transcriptomics data, especially spot-based methods like Visium (55 µm spots capturing multiple cells), often consist of mixture profiles – each spot may contain multiple cell types. **Deconvolution** is the task of determining which cell types (and how much of each) are present in each spot. Deep learning has been applied here by integrating **single-cell RNA-seq references** with spatial data. One prominent tool is **Cell2location** (2022), which uses a Bayesian regression model (built in a probabilistic programming framework) to infer the abundance of reference-defined cell types in each spot. While Cell2location itself isn’t a deep neural network, it uses variational inference (thus some consider it “deep” in the loose sense) and has been widely adopted. It improves over simpler regression by sharing information across genes and spots in a hierarchical model. Another tool, **Tangram** (2020, from the Kosik and Pachter groups), matches single-cell profiles to spatial spots via an optimal transport algorithm; again not a neural network, but it could be paired with one (and indeed some experiments have used a neural network to refine Tangram’s mapping).

Deep learning enters more directly in methods like **GraphST** or **GCN-based deconvolution** where a graph neural network is used to smooth cell type assignment across neighboring spots. There has also been exploration of **domain-adversarial training** to transfer cell type labels from single-cell data to spatial data: a model is trained to embed both scRNA-seq cells and ST spots in the same latent space and align them (somewhat akin to batch integration). An example is **stAligner** (2022), which used a triplet neural network to align single-cell and spatial domains.

Another interesting area is using DL to enhance spatial resolution. **XCNN** (2022) by Fischer *et al.* trained a convolutional network to **upsample** Visium data – essentially predicting gene expression at a higher spatial resolution (smaller grid) by interpolating informed by histology. They treated it like an image super-resolution problem, where the “image” is a set of gene expression channels. Similarly, **SpaGAN** (2021) applied a GAN to generate higher-resolution spatial gene expression maps from low-resolution input, guided by the tissue image (the “generator” imagines what the expression would look like at single-cell resolution, and a “discriminator” tries to distinguish generated from real single-cell data if available). These approaches are still experimental but show how ML can push spatial transcriptomics beyond its raw capabilities.

### Cell–Cell Interaction and Other Downstream Analyses

Once spatial data are integrated and annotated, one may want to infer which cells are interacting (e.g. via ligand-receptor pairs). Several tools (like **CellPhoneDB**, **NICHES**) exist that overlay known ligand-receptor pairs onto spatial proximity. Recently, ML has been used to refine such analyses. The snippet we saw in the SpaGCN article mentions **SpaOTsc** (2020) which uses **optimal transport** to map single-cell data onto spatial positions and then assesses ligand-receptor signaling distances. It even uses a random forest to estimate the effective signaling distance threshold. Another example, **MISTy** (2021), uses multi-view learning (could be thought of as an ML approach) to decompose gene expression into independent spatial influences, identifying signaling sources.

While these are not deep networks, they show ML concepts being applied. In the future, we might see graph neural nets that explicitly model an interaction graph (cells as nodes, potential communication edges) and train on known cell-cell contacts (from multiplex imaging data) to predict new interactions.

### Summary of Spatial Analysis Methods

**Table 3. Selected ML/DL Methods for Spatial Transcriptomics.**

| **Task**                     | **Method (Year)**      | **Approach**                            | **Key Features / Achievements**                                                                                                                                                  |
| ---------------------------- | ---------------------- | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Image & Gene Integration** | *stLearn* (2020)       | Image-driven normalization + clustering | Extracts image features (morphology) to smooth expression (SME). Improved clustering of tissue regions (validated on brain sections).                                            |
|                              | *SpaCell* (2019)       | Dual-branch deep network                | CNN for histology + AE for expression, merged for clustering. Among first to integrate H\&E with ST for domain finding.                                                          |
|                              | *GIST* (2022)          | Joint embedding (CNN + VAE)             | Learns shared latent representation of image and gene data. Aligns modalities to improve downstream analysis (e.g., identifies tumor microenvironments).                         |
|                              | *BrST-Net* (2023)      | CNN/Transformer predicting genes        | Trained deep models (ResNet, ViT) to predict spatial gene expression from H\&E. Predicted 200+ genes with significant correlation, enabling “virtual ST” on new images.          |
|                              | *NePSTA* (2025)        | GNN (graph neural net) for pathology    | Uses GNN on ST data to classify brain tumors. Predicted histological and molecular subtypes with high accuracy, combining image and expression.                                  |
| **Spatial Clustering**       | *SpaGCN* (2021)        | Graph Convolutional Network (GCN)       | Integrates spatial neighbors + histology in a GCN to cluster spots. Identified tissue domains matching manual annotation; outperformed other methods.                            |
|                              | *BayesSpace* (2020)    | Bayesian spatial clustering (no DL)     | (Non-DL) Uses Bayesian mixture model for high-res clustering of Visium spots. Often a baseline for domain detection, included here for context.                                  |
|                              | *SEDR* (2021)          | AE + Variational GAE                    | Low-dim embedding via autoencoder; spatial relationships via graph AE. Yields joint latent space for clustering. Demonstrated on liver and brain ST data.                        |
|                              | *STAGATE* (2022)       | Graph Attention Autoencoder             | GAT to adaptively weight neighbor influence. Achieved state-of-art spatial domain delineation on several Visium datasets, handling irregular point layouts.                      |
| **Deconvolution / Mapping**  | *Cell2location* (2022) | Bayesian regression (variational)       | Learns cell type signatures from scRNA-seq and fits their abundance in ST spots. Not a DNN per se, but a key ML method for integrating single-cell and spatial.                  |
|                              | *Tangram* (2020)       | Mapping via optimal transport           | Maps single-cell-derived cell types to spatial locations, aligning distributions. Often used with DL image features to refine mapping.                                           |
|                              | *STAligner* (2022)     | Triplet network for alignment           | Deep metric-learning to align scRNA and ST spaces. Preserves cell type neighborhood structure in the alignment.                                                                  |
|                              | *XCNN* (2022)          | Convolutional neural net upsampling     | Treats ST as a low-res image; CNN trained to super-resolve gene expression maps. Generates finer spatial maps of expression (validated on simulated mixtures).                   |
| **Spatial Gene Detection**   | *SpaGE* (2020)         | Transfer learning (autoencoder)         | Transfers gene expression patterns from scRNA-seq to ST. Identifies genes with spatial gradients by reconstructing spatial expression from single-cell.                          |
|                              | *SpatialGene* (2021)   | Graph Lasso (ML method)                 | Uses graphical lasso to select genes with spatially correlated expression. (Statistical, but provides feature selection for DL follow-up.)                                       |
| **Cell–Cell Interaction**    | *SpaOTsc* (2020)       | Optimal transport + random forest       | Aligns single-cell data to spatial map; uses random forest to infer ligand–receptor interaction distance. Identified spatially constrained signaling in tumor niches.            |
|                              | *MISTy* (2021)         | Multi-view learning (ML)                | Decomposes expression into spatial, neighbor, and autonomous contributions. Uses an ensemble of regression models to find interactions.                                          |
|                              | *GraphComm (2023)*     | Graph Neural Network for communication  | (Hypothetical recent method) Could model cells as nodes with features, train to predict known interactions, then generalize. (Included as an expected development in the field.) |

*Table 3:* **Notable ML/DL methods in spatial transcriptomics analysis.** We list methods integrating images with transcripts, graph-based spatial clustering, cell type mapping, and others. (Methods in italics are non-deep but are important companions to DL approaches.)

As evidenced by Table 3, deep learning is central to modern spatial omics analysis. In particular, **graph neural networks** and **convolutional networks** allow simultaneous handling of spatial structure and high-dimensional gene data. Researchers have noted that traditional tools often falter on the complexity of spatial data, whereas DL models thrive on multimodal inputs and large data volumes. A 2023 review stated that DL models have proven *“versatile for integrating histology images, gene expression matrices and spatial information”*, handling the complicated SRT datasets and enabling downstream analyses that were previously infeasible. Challenges remain (e.g., data-hungry nature of DL, need for interpretability, and tackling technical issues like batch effects in ST). But given the trajectory, we expect further innovations such as **foundation models for spatial biology** (pretrained on large collections of tissues) and more **hybrid models** combining prior knowledge (e.g. known anatomy or physics of diffusion) with deep networks. The integration of spatial proteomics (imaging mass cytometry) and spatial transcriptomics by DL is another exciting frontier, where models might take in multiple co-registered modality images.

## Deep Learning Model Architectures and Their Roles

Underlying the methods discussed are several core deep learning **architectures**. Here we provide a brief technical overview of the main types of neural network models used in single-cell and spatial analysis, and how they are applied:

### Autoencoders and Variational Autoencoders

**Autoencoders (AEs)** are neural networks that attempt to compress input data into a lower-dimensional latent code and then reconstruct the original data from that code. They consist of an encoder network \$E(x) = z\$ and a decoder network \$D(z) = \hat{x}\$, and are trained to minimize reconstruction error (\$\lVert x - \hat{x}\rVert\$). In single-cell analysis, AEs are popular for **denoising** and **dimensionality reduction** because they can learn nonlinear combinations of gene features. For example, an autoencoder might compress a 20,000-gene expression profile into a 50-dimensional vector that captures the cell’s identity and state, then decompress back to 20,000 genes. As we saw, AEs form the backbone of many methods (DCA, SAUCIE, scDeepCluster, SEDR, etc.). A variant, the **Denoising Autoencoder (DAE)**, includes noise in the input or a penalty to encourage the model to reconstruct a cleaned version of noised input – this is explicitly used in some single-cell methods to handle dropout noise (e.g. DCA adds noise and uses ZINB loss).

**Variational Autoencoders (VAEs)** extend AEs by making the latent space probabilistic. Instead of outputting a single code \$z\$, the encoder produces a distribution (mean and variance) for \$z\$; the model is trained to make this latent distribution approximate a prior (usually Gaussian) while still being informative enough to reconstruct the data. VAEs are powerful for **generative modeling** – you can sample from the latent space to create new data – and for **representation learning** with regularized, continuous latent spaces. In scRNA-seq, the VAE’s probabilistic nature is useful to model the inherent noise (Poisson or NB) in count data. Tools like scVI and scVAE use VAEs with custom likelihoods (ZINB) to better fit count distributions. The KL-divergence regularization in VAEs also acts as a kind of information bottleneck, preventing overfitting and often yielding more meaningful latent dimensions (e.g. one might correspond to a gradation of cell cycle, another to a batch effect, etc., which can be disentangled). One downside is that VAEs can oversmooth and miss rare subpopulations, but techniques like mixture priors or hierarchical VAEs (as in scDHA) have been introduced to mitigate that.

In summary, autoencoders (AEs/VAEs) are **workhorse architectures** in single-cell analysis for tasks requiring compression of high-dimensional data. They enable **unsupervised learning** of structure without needing labeled data. Moreover, once trained, the encoder’s output (cell embeddings) can be fed into downstream tasks (clustering, trajectories, etc.), and the decoder can sometimes be inspected to see which genes are linked to which latent factors (improving interpretability, as done in LDVAE with a linear decoder).

### Graph Neural Networks (GNNs)

Graph neural networks are designed to operate on graph-structured data. In a GNN, each node has features (e.g. a cell’s gene expression) and is connected to other nodes (edges, e.g. cell-cell similarities or spatial neighbors). GNN layers aggregate information from a node’s neighbors to update the node’s representation. This is powerful for any problem where relationships or interactions matter in addition to individual features.

In single-cell biology, GNNs have been utilized in several ways:

* **Cell similarity graphs**: We often build k-Nearest-Neighbor (kNN) graphs of cells based on expression. GNNs can take this graph and perform tasks like **cell type classification** by propagating label information through the graph (a form of semi-supervised learning). For instance, scDeepSort’s GCN uses a bipartite graph of cells and genes to propagate signals and classify cells. Another example is using a GNN to refine clustering: a GraphSAGE or GCN can embed nodes such that connected cells (similar expression) come closer in embedding space than disconnected ones, effectively denoising the graph.
* **Spatial grids/graphs**: As described, methods like SpaGCN, STAGATE use GNNs where nodes are spatial locations (spots or cells) and edges connect neighbors. The GNN naturally enforces that nearby locations influence each other’s representations, producing smoother, spatially-aware outputs.
* **Cell–cell interaction networks**: One could construct a graph of cells where edges represent potential interactions (e.g. a T cell and a nearby antigen-presenting cell). A GNN could then predict some outcome of the interaction or classify the type of interaction by looking at the features of the two cells and their connection. While not yet mainstream, this idea is emerging in computational immunology.
* **Gene regulatory networks**: GNNs aren’t limited to cells as nodes; one could have genes as nodes with edges for known regulatory relationships, and then use gene expression data to predict new connections or important regulators (graph inference). Some attempts have been made (GraphGRN, etc.), although simpler algorithms like GENIE3 remain common for GRN inference.

The common GNN types are Graph Convolutional Networks (GCNs) which do weighted averaging of neighbor features, Graph Attention Networks (GATs) which learn weights (attentions) for each neighbor rather than treating all neighbors equally, and Graph Autoencoders which combine GNNs with an autoencoder objective (encode graph into latent, then maybe decode adjacency or node features).

A concrete example: **SpaGCN** uses a GCN layer where each spot’s features (gene expression vector, possibly reduced by PCA) are updated by a weighted sum of its neighbors’ features. The weights are learned but fixed by the graph structure (like normalized inversely with distance). A multi-layer GCN effectively diffuses gene expression information across the tissue. After a few layers, each node’s embedding contains information from its multi-hop neighborhood. This is then used for clustering. In contrast, **STAGATE** uses GAT so that the importance of each neighbor is learned (maybe a neighbor with a similar histology is given higher weight than one that’s just physically close).

One challenge with GNNs is they can over-smooth (make all node embeddings too similar if too many layers) – but methods often mitigate this by limiting to few layers or adding regularizations. Also, graphs in single-cell can be large (millions of nodes in an atlas), requiring subsampling or minibatch training (graph sampling techniques).

Overall, GNNs bring **relational inductive bias** – they encode the assumption that connected entities should be analyzed together. This is extremely useful in biology, where relationships (physical contact, similarity, lineage) are often as important as the entities themselves.

### Transformers and Attention Mechanisms

Transformers, originally from the NLP domain, rely on self-attention mechanisms to model relationships between elements in a sequence. They have made inroads into genomics in two ways: (1) Treating sequences (like DNA, protein, or immune receptor sequences) as natural language, and (2) Treating a high-dimensional vector (like a cell’s gene expression) as a “set” of features that can attend to each other.

**In immune repertoire analysis**, transformers have shined by treating amino acid sequences of TCRs/BCRs as sentences. The self-attention in models like BERT allows them to capture which amino acids in a CDR3 sequence are interacting (even if far apart in the linear sequence) – effectively it can learn the language of biochemical motifs. **BertTCR** and similar models pretrain on large unlabeled sets of receptor sequences (like millions from healthy repertoires) using masked language modeling (predicting masked amino acids). This teaches the model general sequence features (e.g. the typical motif of a beta chain, or that CASS**L**G is common, etc.). Fine-tuning can then be done for specific tasks: e.g., classify if a repertoire is from a cancer patient, or predict if a TCR binds a given peptide. The attention weights can sometimes be interpreted to see which part of the sequence was important for a prediction, aiding interpretability.

**In single-cell transcriptomics**, the use of transformers is more experimental but rising. One approach called **scBERT** or **scGPT** treats each cell’s gene expression profile as if it were a “sentence” of gene tokens, possibly after discretizing expression levels. For example, one could label each gene as being in one of, say, 10 expression bins and then represent a cell as a sequence of (gene,bin) tokens for highly expressed genes. A transformer could then learn patterns like “if *IL7R* is high and *CCR7* is high and *GZMB* is low, that’s a naive T cell” by attention focusing on those gene tokens together. Another model, **scFormer** (2022), tried splitting the gene vector into chunks and processing with transformers to capture gene–gene interactions. The challenge is that unlike sequences or image patches, a cell’s gene expression has no obvious ordering, so one has to impose an order or treat it as a set (transformers can be adapted to sets by using positional encodings that are all the same or learnable). Despite this, transformers bring the benefit of **global pairwise interactions** – every gene can potentially attend to every other gene in determining the cell representation. This could in theory capture regulatory or co-expression relationships automatically.

A specific example is **TOSICA** (2023) which used a transformer for cell type annotation. They fed in gene expressions and let the model attend to different gene subsets. They found it gave similar accuracy to other DL models but with the bonus that the attention matrix could be queried: for a given cell type output, one could see which genes were most attended to, and those often matched known markers (hence “interpretable” cell type annotation).

Transformers have also been used in multi-omics: for instance, treating a cell’s RNA and ATAC data as two sequences and using cross-attention to integrate them.

One emerging area is **generative transformers** for single-cell, like **scGPT** (2023). scGPT is a GPT-2 style model that was trained to auto-regressively generate gene expression profiles. Essentially it learns the distribution of gene expression in cells across many tissues. Such a model can be used to simulate realistic single-cell data or to impute missing values by “filling in” genes. The authors showed that scGPT’s latent space captured cell types and developmental trajectories, suggesting it had learned some biology across its training corpora. While still a preprint, this hints that large foundation models (with hundreds of millions of parameters) could be trained on single-cell atlases and then fine-tuned for various tasks (similar to how image and text foundation models are used).

**Attention mechanisms** more broadly (not just full transformers) have been very useful, as seen in DeepRC’s repertoire classification and even in some graph models (Graph Attention networks). Attention provides a way to weight the contribution of different features or instances in a data-driven manner. In single-cell, an attention-based model might, for example, attend more to genes that are marker genes for a certain prediction. In spatial, one could imagine attention weighting the influence of surrounding spots based on their expression similarity (effectively learning the neighborhood size).

To summarize, transformers bring the power of **contextual learning** – they consider all elements (be it sequence residues or genes or cells in a batch) and model their interactions. They are data-hungry and computationally heavy, which is why they are just now being explored as datasets become bigger (millions of cells). If trends in NLP and vision are an indicator, we may soon see **pretrained transformer models** for single-cell that can be adapted to many tasks with minimal fine-tuning, which would be a paradigm shift in bioinformatics practice.

### Generative Adversarial Networks (GANs) and Other Models

**GANs** consist of a generator and a discriminator in competition. They have been used in single-cell analysis mainly for **data generation/augmentation** and for **domain adaptation** (as described in batch correction). For data generation, an example is **scGAN** (2018) which tried to generate realistic single-cell gene expression vectors after training on a reference dataset. The idea was to sample many “new” cells from the GAN to augment rare populations or to analyze model learned distribution. However, training GANs on high-dim counts is tricky; VAEs (which explicitly incorporate noise models) have been more stable. Still, **scIGANs** (2020) successfully used a GAN to impute dropouts, essentially by generating neighbor-like cells to fill in missing values.

For **domain adaptation**, GAN-based approaches like **CycleGAN** style have been used to convert one domain’s single-cell data to another (e.g., convert in silico sorted bulk data to single-cell-like distributions). **Adversarial training** more generally (as in AD-AE, scAdapt, etc.) we already covered – it’s basically GAN principles applied to latent spaces.

**Contrastive Learning** is another ML paradigm gaining traction. Here, the model is trained to bring representations of similar inputs closer and push dissimilar ones apart. In single-cell, one application was **contrastiveVI** (2021) which learned an embedding that separates biological signal from technical noise by contrasting cells before/after a perturbation. In spatial, **STAIG** used graph contrastive learning to integrate modalities. Contrastive methods can learn without explicit labels by using data augmentations (like a cell with and without noise should have similar embeddings). This has been used in scNym’s MixMatch (different augmented versions of the same cell’s data should yield the same label).

**Ensemble models** and classical ML: While deep learning dominates the narrative, many pipelines still use gradient boosted trees (XGBoost, Random Forests) for some parts – particularly when features have been engineered. For example, an immune repertoire classifier might compute 100 features (clonality indices, gene usage frequencies) and feed them to XGBoost. These can sometimes outperform deep nets on smaller datasets or when features are well-understood. However, as data scales, deep models that learn features usually take the lead.

**Interpretable models**: There’s a push for interpretability – one method not yet mentioned is **attention-based interpretability in AEs**. For instance, an approach called **Integrated Gradients** can be applied to AEs or classifiers to rank genes by influence on a given latent dimension or prediction. Another approach (in Cell Assign, 2019) built interpretability in by having a model that directly uses known marker genes to assign cell types (though that one wasn’t deep, more Bayesian).

Finally, a quick note on **hardware and scalability**: Single-cell datasets can be huge (millions of cells, or spatial data with gigapixel images). DL methods are leveraging GPUs and often require mini-batch training (sampling cells). Libraries like PyTorch and TensorFlow are widely used; specialized frameworks (Scanpy’s scvi-tools uses Pyro for VAEs, TensorFlow for some deep impute methods, etc.) are making it easier for biologists to apply these models.

## Conclusion and Outlook

In the past five years, we have witnessed a remarkable convergence of single-cell genomics and advanced machine learning. Deep learning models – from autoencoders to transformers – have proven adept at tackling the unique challenges of single-cell and spatial data, pushing the field beyond what was possible with conventional analyses. These models have improved the sensitivity and accuracy of every step in the pipeline: **denoising** data to reveal true biological signal, **integrating** diverse datasets into unified atlases despite batch effects, **clustering** cells into meaningful types and states, and **annotating** those types either by learning from references or from the data itself. In parallel, ML has enriched **immunology analyses**, uncovering latent structure in TCR/BCR repertoires that correlates with infection, autoimmunity, and therapy response. And with spatial transcriptomics, deep models integrating histology and expression have effectively brought “life” to tissue sections, identifying tissue substructures and gene patterns with unprecedented clarity.

A striking theme is the **interdisciplinary nature** of these advances. Techniques originally developed for image recognition or language processing are reshaping bioinformatics. Graph neural networks apply the logic of social networks to cell networks in a tissue; transformers treat genes like words in a sentence of a cell. The result is that biological data is now being analyzed with some of the most powerful algorithms known, enabling discovery of subtle phenomena (e.g. a rare cell type defined by an elusive gene program, or an immune signature of disease hidden in receptor sequences) that were previously undetectable.

Looking forward, we anticipate **further integration** of modalities and methods. Single-cell multi-omics (joint measurements of RNA, ATAC, protein, etc.) will benefit from multi-encoder, multi-decoder architectures that can learn a shared representation from all data types (some early examples exist, like multi-modal VAEs). **Federated learning** might come into play, allowing models to be trained on data from multiple institutes without pooling the data (useful for patient data privacy). We also foresee the rise of **pretrained models**: just as image analysis was revolutionized by models like ResNet or transformers pretrained on ImageNet, single-cell analysis may soon have large models pretrained on cell atlases (hundreds of donors, tissues, conditions) that can be fine-tuned for specific projects. The scGPT and foundation model efforts are first steps in this direction.

Another important direction is **interpretability and validation**. As these models influence scientific conclusions (e.g. suggesting a new cell type or interaction), it’s crucial to interpret what the model has learned. Hybrid models that incorporate known biology (e.g. known gene networks or pathways) or provide understandable outputs (attention scores on genes, attribution of predictions to specific sequence motifs, etc.) will build trust and extract knowledge, not just predictions. Already, attention-based models in single-cell allow identification of key genes driving clusters or classifications, and adversarial models isolating batch effects help confirm that what remains in the data is biological.

In conclusion, the synergy between single-cell biology and machine learning has catalyzed a leap in our ability to dissect complex biological systems. Single-cell datasets are no longer just high-dimensional and noisy burdens; they are now fertile ground where deep neural networks can **learn the rules of life’s diversity** – be it how gene expression defines a cell’s identity, how immune receptors encode health status, or how cellular neighborhoods form a tissue. As these methods become more accessible (many are implemented in open-source tools cited above) and more widely adopted, we expect rapid progress in both computational methods and the biological discoveries they enable. The last five years were about proving the concepts and initial applications; the next five could bring routine use of AI in labs for cell annotation, drug response prediction, and real-time pathology – truly bridging the gap from **“Data Tells Us”** to **“AI Shows Us”** in single-cell science.




